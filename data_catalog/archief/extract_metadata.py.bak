import psycopg2
import psycopg2.extras
import yaml
import datetime
import argparse
import logging
from logging.handlers import RotatingFileHandler
import os

# Setup logging with rotation and console output
log_filename = f"catalog_extraction_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
log_handler = RotatingFileHandler(log_filename, maxBytes=5*1024*1024, backupCount=5)
console_handler = logging.StreamHandler()

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[log_handler, console_handler]
)

# Summary counters
summary = {
    'databases_added': 0,
    'schemas_added': 0,
    'tables_added': 0,
    'tables_updated': 0,
    'columns_added': 0,
    'columns_updated': 0
}


def load_config(path='servers_config.yaml'):
    with open(path, 'r') as f:
        return yaml.safe_load(f)


def connect_db(config, dbname=None):
    return psycopg2.connect(
        host=config["host"],
        port=5432,
        dbname=dbname or "postgres",
        user=config["user"],
        password=config["password"]
    )


def get_user_databases(conn):
    with conn.cursor() as cur:
        cur.execute("""
            SELECT datname FROM pg_database
            WHERE datistemplate = false AND datname NOT IN ('postgres');
        """)
        return [row[0] for row in cur.fetchall()]


def get_schemas(conn):
    with conn.cursor() as cur:
        cur.execute("""
            SELECT schema_name FROM information_schema.schemata
            WHERE schema_name NOT IN ('pg_catalog', 'information_schema');
        """)
        return [row[0] for row in cur.fetchall()]


def get_tables(conn, schema):
    with conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:
        cur.execute("""
            SELECT table_name, table_type
            FROM information_schema.tables
            WHERE table_schema = %s;
        """, (schema,))
        return cur.fetchall()


def get_columns(conn, schema, table):
    with conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:
        cur.execute("""
            SELECT column_name, data_type, is_nullable, column_default, ordinal_position
            FROM information_schema.columns
            WHERE table_schema = %s AND table_name = %s;
        """, (schema, table))
        return cur.fetchall()


def upsert_database(cur, dbname, server_ip, now):
    logging.debug(f"Upserting database: {dbname} @ {server_ip}")
    cur.execute("""
        SELECT id FROM metadata.catalog_databases
        WHERE database_name = %s AND server_address = %s AND curr_id = 'Y';
    """, (dbname, server_ip))
    existing = cur.fetchone()

    if not existing:
        logging.info(f"New database detected: {dbname} @ {server_ip}")
        cur.execute("""
            INSERT INTO metadata.catalog_databases (database_name, server_address, date_created, curr_id)
            VALUES (%s, %s, %s, 'Y') RETURNING id;
        """, (dbname, server_ip, now))
        summary['databases_added'] += 1
        return cur.fetchone()[0]
    return existing[0]


def upsert_schema(cur, schema_name, db_id, now):
    logging.debug(f"Upserting schema: {schema_name} in DB ID {db_id}")
    cur.execute("""
        SELECT id FROM metadata.catalog_schemas
        WHERE schema_name = %s AND database_id = %s AND curr_id = 'Y';
    """, (schema_name, db_id))
    existing = cur.fetchone()

    if not existing:
        logging.info(f"New schema detected: {schema_name} in DB ID {db_id}")
        cur.execute("""
            INSERT INTO metadata.catalog_schemas (schema_name, database_id, date_created, curr_id)
            VALUES (%s, %s, %s, 'Y') RETURNING id;
        """, (schema_name, db_id, now))
        summary['schemas_added'] += 1
        return cur.fetchone()[0]
    return existing[0]


def upsert_table(cur, schema_id, table_name, table_type, now):
    logging.debug(f"Upserting table: {table_name} ({table_type}) in schema ID {schema_id}")
    cur.execute("""
        SELECT id, table_type FROM metadata.catalog_tables
        WHERE schema_id = %s AND table_name = %s AND curr_id = 'Y';
    """, (schema_id, table_name))
    existing = cur.fetchone()

    if existing:
        existing_id, existing_type = existing
        if existing_type == table_type:
            return existing_id
        logging.info(f"Table type changed for {table_name}: {existing_type} → {table_type}")
        cur.execute("""
            UPDATE metadata.catalog_tables SET curr_id = 'N', date_updated = %s WHERE id = %s;
        """, (now, existing_id))
        summary['tables_updated'] += 1
    else:
        summary['tables_added'] += 1

    cur.execute("""
        INSERT INTO metadata.catalog_tables (schema_id, table_name, table_type, date_created, curr_id)
        VALUES (%s, %s, %s, %s, 'Y') RETURNING id;
    """, (schema_id, table_name, table_type, now))
    return cur.fetchone()[0]


def upsert_column(cur, table_id, column, now):
    logging.debug(f"Upserting column: {column['column_name']} in table ID {table_id}")
    cur.execute("""
        SELECT id, data_type, is_nullable, column_default, ordinal_position
        FROM metadata.catalog_columns
        WHERE table_id = %s AND column_name = %s AND curr_id = 'Y';
    """, (table_id, column['column_name']))
    existing = cur.fetchone()

    if existing:
        (existing_id, dt, nullable, default, pos) = existing
        changed = (
            dt != column['data_type'] or
            (nullable != (column['is_nullable'] == 'YES')) or
            default != column['column_default'] or
            pos != column['ordinal_position']
        )
        if not changed:
            return existing_id
        logging.info(f"Column changed: {column['column_name']} (type: {dt} → {column['data_type']})")
        cur.execute("""
            UPDATE metadata.catalog_columns SET curr_id = 'N', date_updated = %s WHERE id = %s;
        """, (now, existing_id))
        summary['columns_updated'] += 1
    else:
        summary['columns_added'] += 1

    cur.execute("""
        INSERT INTO metadata.catalog_columns (
            table_id, column_name, data_type, is_nullable, column_default,
            ordinal_position, date_created, curr_id
        ) VALUES (%s, %s, %s, %s, %s, %s, %s, 'Y') RETURNING id;
    """, (
        table_id,
        column['column_name'],
        column['data_type'],
        column['is_nullable'] == 'YES',
        column['column_default'],
        column['ordinal_position'],
        now
    ))
    return cur.fetchone()[0]


def run():
    parser = argparse.ArgumentParser(description="Extract metadata from PostgreSQL databases")
    parser.add_argument('--server', help='Filter by server IP')
    parser.add_argument('--dbname', help='Filter by specific database name')
    args = parser.parse_args()

    config = load_config()
    catalog_conn = psycopg2.connect(**config['catalog_db'])

    for server in config['servers']:
        if args.server and server['host'] != args.server:
            continue

        logging.info(f"Connecting to {server['name']} ({server['host']})")
        try:
            admin_conn = connect_db(server)
            dbs = get_user_databases(admin_conn)

            for dbname in dbs:
                if args.dbname and dbname != args.dbname:
                    continue

                logging.info(f"Scanning database: {dbname}")
                db_conn = connect_db(server, dbname)
                schemas = get_schemas(db_conn)
                now = datetime.datetime.now(datetime.UTC)

                try:
                    with catalog_conn.cursor() as cur:
                        db_id = upsert_database(cur, dbname, server['host'], now)

                        for schema in schemas:
                            schema_id = upsert_schema(cur, schema, db_id, now)

                            tables = get_tables(db_conn, schema)
                            for tbl in tables:
                                table_id = upsert_table(
                                    cur, schema_id, tbl['table_name'], tbl['table_type'], now
                                )

                                columns = get_columns(db_conn, schema, tbl['table_name'])
                                for col in columns:
                                    upsert_column(cur, table_id, col, now)

                        catalog_conn.commit()

                except Exception as e:
                    logging.error(f"Error during upsert for {dbname} on {server['host']}: {e}")
                finally:
                    db_conn.close()

            admin_conn.close()

        except Exception as e:
            logging.error(f"Failed to connect to {server['host']}: {e}")

    catalog_conn.close()
    logging.info("Catalog update complete with version tracking.")
    print("\nSummary:")
    for key, val in summary.items():
        print(f"{key.replace('_', ' ').title()}: {val}")



if __name__ == '__main__':
    run()
